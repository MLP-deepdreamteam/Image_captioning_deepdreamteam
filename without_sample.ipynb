{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEHbwgNOJZ1ftn8/G93XPz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLP-deepdreamteam/Practice_captioning/blob/branch_%EB%B0%B1%EC%9A%B4%EC%B2%A0/without_sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.8.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck8h9XNniHXF",
        "outputId": "364f3ad1-1b29-4935-a9b1-b3123d74cc01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.11.0\n",
            "Uninstalling tensorflow-2.11.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.9/dist-packages/tensorflow-2.11.0.dist-info/*\n",
            "    /usr/local/lib/python3.9/dist-packages/tensorflow/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.8.3\n",
            "  Downloading tensorflow-2.8.3-cp39-cp39-manylinux2010_x86_64.whl (498.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.5/498.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (23.3.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (1.22.4)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (15.0.6.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (3.19.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (1.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (3.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (2.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (63.4.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (0.31.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (3.3.0)\n",
            "Collecting tensorflow-estimator<2.9,>=2.8\n",
            "  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 KB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (1.51.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (4.5.0)\n",
            "Collecting tensorboard<2.9,>=2.8\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.9,>=2.8.0rc0\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (0.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.3) (0.40.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (2.2.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (2.16.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (2.27.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (3.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (6.0.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, keras-preprocessing, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.11.0\n",
            "    Uninstalling tensorflow-estimator-2.11.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.11.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.11.0\n",
            "    Uninstalling keras-2.11.0:\n",
            "      Successfully uninstalled keras-2.11.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.11.2\n",
            "    Uninstalling tensorboard-2.11.2:\n",
            "      Successfully uninstalled tensorboard-2.11.2\n",
            "Successfully installed keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorflow-2.8.3 tensorflow-estimator-2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZsKMnQWQJAH",
        "outputId": "cd0369ab-42aa-4807-a60f-72e452b50010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-20 05:44:27--  http://images.cocodataset.org/zips/train2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.137.97, 52.216.249.236, 52.217.95.233, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.137.97|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19336861798 (18G) [application/zip]\n",
            "Saving to: ‘coco_train2017.zip’\n",
            "\n",
            "coco_train2017.zip   58%[==========>         ]  10.61G  20.3MB/s    eta 5m 27s "
          ]
        }
      ],
      "source": [
        "!wget http://images.cocodataset.org/zips/train2017.zip -O coco_train2017.zip\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip -O coco_val2017.zip\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -O coco_ann2017.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "\n",
        "tensorflow.__version__"
      ],
      "metadata": {
        "id": "eSdZjaCLfyGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile, BadZipFile\n",
        "import os\n",
        "def extract_zip_file(extract_path):\n",
        "    try:\n",
        "        with ZipFile(extract_path+\".zip\") as zfile:\n",
        "            zfile.extractall(extract_path)\n",
        "        # remove zipfile\n",
        "        zfileTOremove=f\"{extract_path}\"+\".zip\"\n",
        "        if os.path.isfile(zfileTOremove):\n",
        "            os.remove(zfileTOremove)\n",
        "        else:\n",
        "            print(\"Error: %s file not found\" % zfileTOremove)    \n",
        "    except BadZipFile as e:\n",
        "        print(\"Error:\", e)\n",
        "extract_train_path = \"./coco_train2017\"\n",
        "extract_val_path = \"./coco_val2017\"\n",
        "extract_ann_path=\"./coco_ann2017\"\n",
        "extract_zip_file(extract_train_path)\n",
        "extract_zip_file(extract_val_path)\n",
        "extract_zip_file(extract_ann_path)"
      ],
      "metadata": {
        "id": "aFPuPOzQQQhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from collections import defaultdict\n",
        "# import json\n",
        "# import numpy as np\n",
        "\n",
        "# class COCOParser:\n",
        "#     def __init__(self, anns_file, imgs_dir):\n",
        "#         with open(anns_file, 'r') as f:\n",
        "#             coco = json.load(f)\n",
        "            \n",
        "#         self.annIm_dict = defaultdict(list)        \n",
        "#         self.cat_dict = {} \n",
        "#         self.annId_dict = {}\n",
        "#         self.im_dict = {}\n",
        "#         self.licenses_dict = {}\n",
        "\n",
        "#         for ann in coco['annotations']:           \n",
        "#             self.annIm_dict[ann['image_id']].append(ann) \n",
        "#             self.annId_dict[ann['id']]=ann\n",
        "#         for img in coco['images']:\n",
        "#             self.im_dict[img['id']] = img\n",
        "#         for cat in coco['categories']:\n",
        "#             self.cat_dict[cat['id']] = cat\n",
        "#         for license in coco['licenses']:\n",
        "#             self.licenses_dict[license['id']] = license\n",
        "\n",
        "#     def get_imgIds(self):\n",
        "#         return list(self.im_dict.keys())\n",
        "\n",
        "#     def get_annIds(self, im_ids):\n",
        "#         im_ids=im_ids if isinstance(im_ids, list) else [im_ids]\n",
        "#         return [ann['id'] for im_id in im_ids for ann in self.annIm_dict[im_id]]\n",
        "\n",
        "#     def load_anns(self, ann_ids):\n",
        "#         im_ids=ann_ids if isinstance(ann_ids, list) else [ann_ids]\n",
        "#         return [self.annId_dict[ann_id] for ann_id in ann_ids]        \n",
        "\n",
        "#     def load_cats(self, class_ids):\n",
        "#         class_ids=class_ids if isinstance(class_ids, list) else [class_ids]\n",
        "#         return [self.cat_dict[class_id] for class_id in class_ids]\n",
        "\n",
        "#     def get_imgLicenses(self,im_ids):\n",
        "#         im_ids=im_ids if isinstance(im_ids, list) else [im_ids]\n",
        "#         lic_ids = [self.im_dict[im_id][\"license\"] for im_id in im_ids]\n",
        "#         return [self.licenses_dict[lic_id] for lic_id in lic_ids]"
      ],
      "metadata": {
        "id": "XisfA8tLRbnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coco_annotations_file=\"/content/coco_ann2017/annotations/instances_val2017.json\"\n",
        "# coco_images_dir=\"/content/coco_val2017/val2017\"\n",
        "# coco= COCOParser(coco_annotations_file, coco_images_dir)\n"
      ],
      "metadata": {
        "id": "77CiTb3lSW1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import random\n",
        "import requests\n",
        "import json\n",
        "from math import sqrt\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "BASE_PATH = \"/content/coco_ann2017\"\n",
        "BASE_PATH2 = \"/content/coco_train2017\"\n",
        "with open(f'{BASE_PATH}/annotations/captions_train2017.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "    data = data['annotations']\n",
        "\n",
        "\n",
        "# data[:5]\n",
        "\n",
        "img_cap_pairs = []\n",
        "\n",
        "for sample in data:\n",
        "    img_name = '%012d.jpg' % sample['image_id']\n",
        "    img_cap_pairs.append([img_name, sample['caption']])\n",
        "\n",
        "captions = pd.DataFrame(img_cap_pairs, columns=['image', 'caption'])\n",
        "captions['image'] = captions['image'].apply(\n",
        "    lambda x: f'{BASE_PATH2}/train2017/{x}' # path to image dataset\n",
        ")\n",
        "captions = captions.sample(70000)\n",
        "captions = captions.reset_index(drop=True)\n",
        "captions.head()"
      ],
      "metadata": {
        "id": "FEPPQTacSeAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    text = '[start] ' + text + ' [end]'\n",
        "    return text"
      ],
      "metadata": {
        "id": "oucj4F5ISjoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "captions['caption'] = captions['caption'].apply(preprocess)\n",
        "captions.head()"
      ],
      "metadata": {
        "id": "CFYGiYA5UPv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_row = captions.sample(1).iloc[0]\n",
        "print(random_row.caption)\n",
        "print()\n",
        "im = Image.open(random_row.image)\n",
        "im"
      ],
      "metadata": {
        "id": "DlBQatfGVMPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 40\n",
        "VOCABULARY_SIZE = 15000\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "EMBEDDING_DIM = 512\n",
        "UNITS = 512\n",
        "EPOCHS = 5"
      ],
      "metadata": {
        "id": "ZzKZWAuJVPNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCABULARY_SIZE,\n",
        "    standardize=None,\n",
        "    output_sequence_length=MAX_LENGTH)\n",
        "\n",
        "tokenizer.adapt(captions['caption'])"
      ],
      "metadata": {
        "id": "HUu1QbJJWaf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uZVhjXisiAk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocabulary_size()\n"
      ],
      "metadata": {
        "id": "XQL2T7FjWcBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(tokenizer.get_vocabulary(), open('vocab_coco.file', 'wb'))"
      ],
      "metadata": {
        "id": "0qZNESDZW1Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary())\n",
        "\n",
        "idx2word = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary(),\n",
        "    invert=True)"
      ],
      "metadata": {
        "id": "yAyt2qoTW4Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_to_cap_vector = collections.defaultdict(list)\n",
        "for img, cap in zip(captions['image'], captions['caption']):\n",
        "    img_to_cap_vector[img].append(cap)\n",
        "\n",
        "img_keys = list(img_to_cap_vector.keys())\n",
        "random.shuffle(img_keys)\n",
        "\n",
        "slice_index = int(len(img_keys)*0.8)\n",
        "img_name_train_keys, img_name_val_keys = (img_keys[:slice_index], \n",
        "                                          img_keys[slice_index:])\n",
        "\n",
        "train_imgs = []\n",
        "train_captions = []\n",
        "for imgt in img_name_train_keys:\n",
        "    capt_len = len(img_to_cap_vector[imgt])\n",
        "    train_imgs.extend([imgt] * capt_len)\n",
        "    train_captions.extend(img_to_cap_vector[imgt])\n",
        "\n",
        "val_imgs = []\n",
        "val_captions = []\n",
        "for imgv in img_name_val_keys:\n",
        "    capv_len = len(img_to_cap_vector[imgv])\n",
        "    val_imgs.extend([imgv] * capv_len)\n",
        "    val_captions.extend(img_to_cap_vector[imgv])"
      ],
      "metadata": {
        "id": "IzVlMZmYW4sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_imgs), len(train_captions), len(val_imgs), len(val_captions)\n"
      ],
      "metadata": {
        "id": "oTE5OVpGW6gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(img_path, caption):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    caption = tokenizer(caption)\n",
        "    return img, caption"
      ],
      "metadata": {
        "id": "gJluhrbMW8J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_imgs, train_captions))\n",
        "\n",
        "train_dataset = train_dataset.map(\n",
        "    load_data, num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (val_imgs, val_captions))\n",
        "\n",
        "val_dataset = val_dataset.map(\n",
        "    load_data, num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "mg1dn1RoW9TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_augmentation = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "        tf.keras.layers.RandomRotation(0.2),\n",
        "        tf.keras.layers.RandomContrast(0.3),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "rPagPBSFXCjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN_Encoder(): # Inception\n",
        "    inception_v3 = tf.keras.applications.InceptionV3(\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "\n",
        "    output = inception_v3.output\n",
        "    output = tf.keras.layers.Reshape(\n",
        "        (-1, output.shape[-1]))(output)\n",
        "\n",
        "    cnn_model = tf.keras.models.Model(inception_v3.input, output)\n",
        "    return cnn_model"
      ],
      "metadata": {
        "id": "eIeCGl21XC5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n",
        "        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n",
        "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense = tf.keras.layers.Dense(embed_dim, activation=\"relu\")\n",
        "    \n",
        "\n",
        "    def call(self, x, training):\n",
        "        x = self.layer_norm_1(x)\n",
        "        x = self.dense(x)\n",
        "\n",
        "        attn_output = self.attention(\n",
        "            query=x,\n",
        "            value=x,\n",
        "            key=x,\n",
        "            attention_mask=None,\n",
        "            training=training\n",
        "        )\n",
        "\n",
        "        x = self.layer_norm_2(x + attn_output)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vHnMLbDRXE9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, max_len):\n",
        "        super().__init__()\n",
        "        self.token_embeddings = tf.keras.layers.Embedding(\n",
        "            vocab_size, embed_dim)\n",
        "        self.position_embeddings = tf.keras.layers.Embedding(\n",
        "            max_len, embed_dim, input_shape=(None, max_len))\n",
        "    \n",
        "\n",
        "    def call(self, input_ids):\n",
        "        length = tf.shape(input_ids)[-1]\n",
        "        position_ids = tf.range(start=0, limit=length, delta=1)\n",
        "        position_ids = tf.expand_dims(position_ids, axis=0)\n",
        "\n",
        "        token_embeddings = self.token_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "\n",
        "        return token_embeddings + position_embeddings"
      ],
      "metadata": {
        "id": "txn_8bsRXKQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, embed_dim, units, num_heads):\n",
        "        super().__init__()\n",
        "        self.embedding = Embeddings(\n",
        "            tokenizer.vocabulary_size(), embed_dim, MAX_LENGTH)\n",
        "\n",
        "        self.attention_1 = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
        "        )\n",
        "        self.attention_2 = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
        "        )\n",
        "\n",
        "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm_3 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.ffn_layer_1 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
        "        self.ffn_layer_2 = tf.keras.layers.Dense(embed_dim)\n",
        "\n",
        "        self.out = tf.keras.layers.Dense(tokenizer.vocabulary_size(), activation=\"softmax\")\n",
        "\n",
        "        self.dropout_1 = tf.keras.layers.Dropout(0.3)\n",
        "        self.dropout_2 = tf.keras.layers.Dropout(0.5)\n",
        "    \n",
        "\n",
        "    def call(self, input_ids, encoder_output, training, mask=None):\n",
        "        embeddings = self.embedding(input_ids)\n",
        "\n",
        "        combined_mask = None\n",
        "        padding_mask = None\n",
        "        \n",
        "        if mask is not None:\n",
        "            causal_mask = self.get_causal_attention_mask(embeddings)\n",
        "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
        "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
        "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
        "\n",
        "        attn_output_1 = self.attention_1(\n",
        "            query=embeddings,\n",
        "            value=embeddings,\n",
        "            key=embeddings,\n",
        "            attention_mask=combined_mask,\n",
        "            training=training\n",
        "        )\n",
        "\n",
        "        out_1 = self.layernorm_1(embeddings + attn_output_1)\n",
        "\n",
        "        attn_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_output,\n",
        "            key=encoder_output,\n",
        "            attention_mask=padding_mask,\n",
        "            training=training\n",
        "        )\n",
        "\n",
        "        out_2 = self.layernorm_2(out_1 + attn_output_2)\n",
        "\n",
        "        ffn_out = self.ffn_layer_1(out_2)\n",
        "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
        "        ffn_out = self.ffn_layer_2(ffn_out)\n",
        "\n",
        "        ffn_out = self.layernorm_3(ffn_out + out_2)\n",
        "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
        "        preds = self.out(ffn_out)\n",
        "        return preds\n",
        "\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0\n",
        "        )\n",
        "        return tf.tile(mask, mult)"
      ],
      "metadata": {
        "id": "FyquDW9lXKxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCaptioningModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, cnn_model, encoder, decoder, image_aug=None):\n",
        "        super().__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.image_aug = image_aug\n",
        "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "        self.acc_tracker = tf.keras.metrics.Mean(name=\"accuracy\")\n",
        "\n",
        "\n",
        "    def calculate_loss(self, y_true, y_pred, mask):\n",
        "        loss = self.loss(y_true, y_pred)\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        loss *= mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
        "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
        "        accuracy = tf.math.logical_and(mask, accuracy)\n",
        "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
        "        mask = tf.cast(mask, dtype=tf.float32)\n",
        "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
        "    \n",
        "\n",
        "    def compute_loss_and_acc(self, img_embed, captions, training=True):\n",
        "        encoder_output = self.encoder(img_embed, training=True)\n",
        "        y_input = captions[:, :-1]\n",
        "        y_true = captions[:, 1:]\n",
        "        mask = (y_true != 0)\n",
        "        y_pred = self.decoder(\n",
        "            y_input, encoder_output, training=True, mask=mask\n",
        "        )\n",
        "        loss = self.calculate_loss(y_true, y_pred, mask)\n",
        "        acc = self.calculate_accuracy(y_true, y_pred, mask)\n",
        "        return loss, acc\n",
        "\n",
        "    \n",
        "    def train_step(self, batch):\n",
        "        imgs, captions = batch\n",
        "\n",
        "        if self.image_aug:\n",
        "            imgs = self.image_aug(imgs)\n",
        "        \n",
        "        img_embed = self.cnn_model(imgs)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss, acc = self.compute_loss_and_acc(\n",
        "                img_embed, captions\n",
        "            )\n",
        "    \n",
        "        train_vars = (\n",
        "            self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "        )\n",
        "        grads = tape.gradient(loss, train_vars)\n",
        "        self.optimizer.apply_gradients(zip(grads, train_vars))\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(acc)\n",
        "\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "    \n",
        "\n",
        "    def test_step(self, batch):\n",
        "        imgs, captions = batch\n",
        "\n",
        "        img_embed = self.cnn_model(imgs)\n",
        "\n",
        "        loss, acc = self.compute_loss_and_acc(\n",
        "            img_embed, captions, training=False\n",
        "        )\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(acc)\n",
        "\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.acc_tracker]"
      ],
      "metadata": {
        "id": "WTGaaMNfXQ3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = TransformerEncoderLayer(EMBEDDING_DIM, 1)\n",
        "decoder = TransformerDecoderLayer(EMBEDDING_DIM, UNITS, 8)\n",
        "\n",
        "cnn_model = CNN_Encoder()\n",
        "caption_model = ImageCaptioningModel(\n",
        "    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,\n",
        ")"
      ],
      "metadata": {
        "id": "uVcGGFTEXRfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False, reduction=\"none\"\n",
        ")\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "caption_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=cross_entropy\n",
        ")"
      ],
      "metadata": {
        "id": "FCX44AhDXTVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history = caption_model.fit(\n",
        "#     train_dataset,\n",
        "#     epochs=EPOCHS,\n",
        "#     validation_data=val_dataset,\n",
        "#     callbacks=[early_stopping]\n",
        "# )\n",
        "\n",
        "# WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
        "# 같은 경고문 뜬다면 tf 2.8.3으로 downgrade 할것\n",
        "\n"
      ],
      "metadata": {
        "id": "jfCAAucWXWid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ovw_wJNcdruA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = caption_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ],
      "metadata": {
        "id": "tZEe9abtc46n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fjhczmqJXXw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_from_path(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def generate_caption(img_path, add_noise=False):\n",
        "    img = load_image_from_path(img_path)\n",
        "    \n",
        "    if add_noise:\n",
        "        noise = tf.random.normal(img.shape)*0.1\n",
        "        img = img + noise\n",
        "        img = (img - tf.reduce_min(img))/(tf.reduce_max(img) - tf.reduce_min(img))\n",
        "    \n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    img_embed = caption_model.cnn_model(img)\n",
        "    img_encoded = caption_model.encoder(img_embed, training=False)\n",
        "\n",
        "    y_inp = '[start]'\n",
        "    for i in range(MAX_LENGTH-1):\n",
        "        tokenized = tokenizer([y_inp])[:, :-1]\n",
        "        mask = tf.cast(tokenized != 0, tf.int32)\n",
        "        pred = caption_model.decoder(\n",
        "            tokenized, img_encoded, training=False, mask=mask)\n",
        "        \n",
        "        pred_idx = np.argmax(pred[0, i, :])\n",
        "        pred_idx = tf.convert_to_tensor(pred_idx)\n",
        "        pred_word = idx2word(pred_idx).numpy().decode('utf-8')\n",
        "        if pred_word == '[end]':\n",
        "            break\n",
        "        \n",
        "        y_inp += ' ' + pred_word\n",
        "    \n",
        "    y_inp = y_inp.replace('[start] ', '')\n",
        "    return y_inp"
      ],
      "metadata": {
        "id": "OOSVDAYpXZvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = random.randrange(0, len(captions))\n",
        "img_path = captions.iloc[idx].image\n",
        "\n",
        "pred_caption = generate_caption(img_path)\n",
        "print('Predicted Caption:', pred_caption)\n",
        "print()\n",
        "Image.open(img_path)"
      ],
      "metadata": {
        "id": "kCeptwx3Xdf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7f-8SVcqancM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}